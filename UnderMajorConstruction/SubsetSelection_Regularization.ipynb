{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset Selection, Shrinkage and Dimension Reductions: Improving upon Linear Regression Models \n",
    "In the following sections we will discuss some alternative metrics used to choose an 'optimal' linear regression model other than least squares. Some of these methods still leverage the least square metric but they are not solely based on the formulas we have derived previously up above.\n",
    "\n",
    "Some reasons from using other metrics:  \n",
    "(1) Least squares when the number of predictor variables is relatively large compared to the number of data points you are training on can lead to a high variance in coefficients and thus poor performance on 'test' data.\n",
    "(2) If we only rely on least squares to pick a model given a set of predictor variables, it's very unlikely that the optimal parameter values that minimizes the least squares error will set any of these parameters to zero. Why is this potentially a problem? If we selected too many irrelevant predictor variables this could muddy up our model and actually hurt it's performance. Thus, if we add some other metrics on top of least squares we can cull some of these extra predictor variables to improve performance.  \n",
    "\n",
    "As mentioned before below are a couple reasons why too many predictor variables is a bad thing:\n",
    "(1) The more predictor variables you have in your model the less interpretable it becomes. Instead of just being able to interpret a simple regression model as given some baby tree, we think that per year it's height will increase by X feet. Now we have to think how incorporating all those predictor variables what it translates to a model that can be explained and understand by the people using it. This is especially important when the goal is not just to *predict* a value but also if we're using the regression model to try to understand the causes behind a certain phenomenon. \n",
    "(2) The predictor variables you add the higher chance of creating an overfit model. We haven't discussed in detail what it means for a model to be overfit but basically this means a model that has parameters that have been set to values that result in very low error rate on the training set but it has fit itself much too closely to all the specific random oddities that happen to appear in that specific training dataset such that when used in the future it actually performs very poorly on new data.\n",
    "\n",
    "The below sections touch on a few methods on how to perform subset selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset Selection: Best Subset and stepwise selection\n",
    "\n",
    "All the below selection methods assumes we have 'p' predictor variables. Note: I have coded up each of these subset selection methods to help explain their methodology __ADD CODE BELOW THIS SECTION THAT DOES EACH OF THESE METHODS__\n",
    "\n",
    "__Best Subset Selection__:  \n",
    "In summary this method looks at every possible combination involving any number of 'p' predictor variables (how many possibilities is that?). We take the best model, based on lowest RSS, using 'k' predictor variables with 'k' being 0 through 'p' (thus we end up with p+1 models as our finalists). We then evaluate each of these p+1 models using some other metric like cross-validation error, Mallows Cp, AIC, BIC or adjusted $R^2$\n",
    "\n",
    "This method is attractive because it's simple and considers all possible models. A big con of this method is once the number of predictor variables increase (even just up to 20) the number of models we must consider and perform least squares on increases exponentially. You can see why this is by realizing that the number of models involving any number of the 'p' variables is actually just the power set of P where P is the set of all the predictor variables.\n",
    "\n",
    "__Forward Stepwise Selection__:\n",
    "Basically, it's best subset selection but computationally scalable / more efficient. It accomplishes it this by only looking at 'p-k' models every iteration instead of ${p \\choose k}$. We \"build\" up to a model that has 'p' parameters. And we do this by adding the best remaining predictor variables not already chosen in a previous iteration that gives us the lowest RSS / R-squared(__PROBABLY EDIT AS MIGHT BE CONFUSING - because this makes it sound like the parameter coefficients for the 0-(k-1) parameters are exactly the same where in reality with each iteration they change because we do a least squares optimization on all the relevant training data and thus most likely will end up with different parameter coefficient values__). Thus we look at only 'p-k' models every iteration since we only look at the *remaining* 'p-k' variables that we haven't already added . \n",
    "\n",
    "We start with a null model (no predictor variables) then we look at p-k (therefore just 'p' since k=0 in the first iteration) of the predictor variables and pick the best one predictor model out of these 'p' models based on RSS / highest $R^2$ (__ WHY IS THIS EQUIVALENT?? Lowest RSS and highest R-squared?? A: Because look at the equation for $R^2$ it is $r^2 = 1 - \\frac{RSS}{SSTO}$ Where SSTO can be defined as $SSTO = \\sum_{i=1}^n(y_i - \\bar{y})^2$. THEREFORE, as RSS decreases the second term that subtracts from 1 gets smaller and hence $R^2$ increases. Note: the fraction $\\frac{RSS}{SSTO}$ is between 0 and 1 __) and add it to our existing model. This is now our *contender representative* for a model with 'k' predictor variables.  We then evaluate p-k models next iteration and so on until we have p+1 contending models. We then, similarly to __best subset selection__ evaluate these models with some metric like cross-validation error, Mallows Cp, AIC, BIC or adjusted $R^2$.\n",
    "\n",
    "This is good because instead of evaluating $2^p$ possible models (and doing all the least sqaures optimization to find parameter coefficient values) we only evaluate $\\frac{p+1}{2}*p$ models. (__top of pg. 208 CHECK in textbook have 2 differences from my thinking (1) that summation from k=0 to k=p-1 is that value i wrote. (2) they add 1 to the summation__).\n",
    "\n",
    "__QUESTION - pg. 208/209 footnote ISL says forward/backward selection perform a guided search of model space. what is that? And why does it mean the effective model search space is larger than the summation value?? A: this is just catchy machine learning jargon that doesn't really mean anything. The forward/backward selection is just a label of how the algorithm goes about choosing what it thinks is the best model. The 'effective model search space' again is some loose terminology that doesn't really mean anything. I think what they're trying to say is because of the metrics used in the search algorithm it SHOULD avoid searching in spaces within the model search space that are DEFINITELY going to be worse than what it chooses so by it's logic / metrics it 'effectively' searches more because it's not supposed to look at subspaces that basically have high likelihood of sucking__\n",
    "\n",
    "__Backward Stepwise Selection__:\n",
    "Same as forward selection except we start with a *full* model containing all 'p' predictor variables and we perform least squares optmization to obtain the parameter coefficient values. Then we obtain our *contender* for the model with p-1 variables by taking out the 'least' important variable by using RSS to compare each of the resulting p models that only contain p-1 predictor variables and choosing the model with the lowest RSS. We do this until we get to the null model. We then evaluate all p+1 models the same as above\n",
    "\n",
    "Note: that both best subset and backward selection require that at least n > p. As in order to evaluate a model with 'p' predictor variables you need at least 'p' data points. (__CHRIS__ This is true because in order to solve for all 'p' coefficient values is just a system of linear equations. And thus if you think about it in matrix form if we have n < p that means we have less equations than variables and thus if we put this into matrix form this means that will have more columns than rows and thus the coefficient matrix will have at most fewer pivots than 'p' and thus cannot solve for every coefficient value __CHECK IF THIS REASONING IS CORRECT MATH REASONING A: basically you end up underfitting the data. Haven't confirmed math stuff yet__) However, forward stepwise you don't need n > p as since you start with 0 the algorithm will just stop once it cannot perform least squares optimization. All 3 methods are good solutions when 'p' is relatively large compared to 'n' though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add code loops that execute each of these selection methods.Probably use some kaggle dataset that has a deece number\n",
    "# of predictor variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tie in stat sig notebook and show how you could then run some significance testing on the subset models you end up with and have some sort of heuristic of picking a subset model given the results of the significance tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization methods: Lasso and Ridge - alternative to subset selection\n",
    "These methods can help with a model's performance by shrinking (__THIS IS BASICALLY WHAT REGUARLIZATION IS RIGHT? Moving values down to a smaller scale?__) the parameter coeffcient values. Why does this help? It helps models that suffer from high variances for some or all of their parameter coefficients. And in 'shrinking' these coefficients (in Lasso's case sometimes to zero) we are performing __variable selection__.\n",
    "\n",
    "## Ridge Regression:\n",
    "\n",
    "$$\\sum_{i=1}^n (y_i - \\beta_0 - \\sum_{j=1}^p \\beta_jx_{ij})^2 + \\lambda\\sum_{j=1}^p\\beta_j^2 = RSS + \\lambda\\sum_{j=1}^p\\beta_j^2$$\n",
    "\n",
    "Or in matrix form (remember that error for each y term can be expresses as $(\\vec{y} - X\\vec{\\beta}$ therefore the RSS in vector-matrix form is $(\\vec{y} - X\\vec{\\beta}^T(\\vec{y} - X\\vec{\\beta}$):  \n",
    "\n",
    "$$(\\vec{y} - X\\vec{\\beta})^T(\\vec{y} - X\\vec{\\beta}) + \\lambda\\vec{\\beta}^T\\vec{\\beta}$$\n",
    "\n",
    "We minimize the above function with respect to the betas to find them out, very similar to what we did with least squares optimization except we've add that trailing 'penalty term'. In fact, if we set $\\lambda = 0$ we get exactly least squares solution. However, the more we increase $\\lambda$ the greater the second term will for large $\\beta$ values and thus larger penalty when picking high $\\beta$ values. Basically, what we're doing when we 'regularize' these coefficients is forcing them to be smaller values (as long as $\\lambda > 0$.  \n",
    "\n",
    "This is beneficial as this has a positive impact of the variance of each parameter ($\\beta$) coefficient. Smaller coefficient values are going to have smaller variance values (__CHRIS - why do smaller valued coefficients have smaller variance? Seems simple but math behind it?__). Then as we tweak $\\lambda$ we can use a technique like cross-validation to choose a model that optimizes that variance-bias tradeoff to get us a model that performs better than if we had just used least square optimization. There is a trade off with bias of a model a by now effectively putting a contraint on the model space we can search (i.e only models with lower $\\beta$ valeus) and thus being further 'away' from the actual relationship.\n",
    "\n",
    "Note: Ridge regression will still include all given 'p' predictor variables in its model, just with lower values parameter coefficients. __NEED (CHRIS - same as question below) to address along with why lasso more likely to send coefficients all the way down to zero__\n",
    "\n",
    "## Lasso:\n",
    "\n",
    "$$ \\sum_{i=1}^n \\Big(y_i - \\beta_0 - \\sum_{j=1}^p \\beta_jx_{ij}\\Big)^2 + \\lambda\\sum_{j=1}^p|\\beta_j| = RSS + \\lambda\\sum_{j=1}^p |\\beta_j|$$\n",
    "\n",
    "This optimization function is very similar to ridge regression's. The only difference is that instead of using L2 norm in the penalty term we use the L1 norm. This in effect we end up sending some $\\beta$ values all the way down to zero. Thus, some see this as an advantage when we want to actually decrease the number of predictor variables within a model - in order to simplify it from overfitting or just to improve interpretablilty. __TODO (CHRIS - same as question above) need to talk about why sends parameters to zero. Don't understand ISL's explanation on pg 222__.\n",
    "\n",
    "\n",
    "## Lasso vs. Ridge Regression\n",
    "- As you see from Lasso, this model performs variable selection (like best subset selection does) - which is the process of selecting a subset of the given predictor variables that is hopefully easier to interpret and less prone to overfitting. While Ridge Regression doesn't as it basically always maintains nonzero coefficients for every predictor variable.\n",
    "- (__CHRIS pg.225 explanation of soft-thresholding - would be interested in how maybe doing out 6.14 and 6.15 and maybe at least trying to look at the general X case vs. the case of the identity matrix__)\n",
    "- __TODO: show similar example in ISL, where you compare bias, variance and test MSE and R^2 between Lasso and Ridge__\n",
    "- Because Lasso tends to send some parameter coefficients to zero, it implicity assumes that not all of the predictor variables fed into the model are significant - thus tends to do a better job when in reality a smaller subset of the predictor variables actually determine the response variable and some of the given predictor variables aren't at all part of the true function of the response variable. Ridge Regression implicitly assumes that all given predictor variables are part of the true function that makes up the response variable value as it rarely/ever sends a coefficient value to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (blogPosts)",
   "language": "python",
   "name": "blogposts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
