{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why regularization decreases the variance of a model\n",
    "\n",
    "Remember that Mean Squared Error of a model is:  \n",
    "\n",
    "$$ MSE = \\sigma^2 + Bias^2(\\hat{f}(x)) + Var(\\hat{f}(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We talked about how regularization techniques like Lasso and Ridge Regression constrain the parameter cofficients and thus decrease variance. A good way to think about what this is the case is to first think about high level and how constraining the $\\beta$ values in our $\\hat{f}$ will impact variance and bias.  \n",
    "\n",
    "### Variance  \n",
    "- As we regularize beta values (constrain them to be smaller values per the objective functions of Ridge and Lasso), we are restricting our model space. Because before with just regular RSS we are free to adjust $\\beta$ values as necessary as long as they move us toward a smaller RSS value - this allows us 'access' to the complete model space given those chosen $\\beta$'s. But with constrained values we restrict that space to a subspace that excludes any points in the model space such that the sum of the $\\beta$ values are too high - this in effect decreases our $Var(f) = E_D[(f_d(x) - \\bar{f}(x))^2]$ because in general both the values of $f_d(x)$ and $\\bar{f}$ will decrease together (by around the same factor. And therefore, their absolute difference and hence their squared difference will decrease) and thus the expected value of the difference of the two will decrease. Intuitively, you can think of it as since we have fewer $\\beta$ choices to choose from the model 'f' that we end up depending on a specific training set will vary less just because we have less choices of models to choose from given the constraints put on our objective function (this magnitude of constraint is controlled by $\\lambda$).  \n",
    "- Definition of bias is $Bias(f) = \\bar{f}(x) - y(x)$. Thus, if we think of regularizing the $\\beta$ values as either decreasing the impact of some of them or completely negating their impact that our model will necessarily predict values further from the true function. This is true because the more independent variables you add to a model (whether or not they are actually relevant or not), we could potentially tweak them in a way to better fit the data and thus our model space probably is 'closer' to the true function. And when we constrained our $\\beta$ values that will inherently move the model space away from the true function assuming the constrained space doesn't happen to encompass the true function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the Ridge Regression objective function:  \n",
    "\n",
    "$$\\vec{\\epsilon} = \\vec{y} - X\\vec{\\beta}$$  \n",
    "\n",
    "Therefore, RSS is just $\\vec{\\epsilon}^T\\vec{\\epsilon}$ which equals:  \n",
    "\n",
    "$$RSS = (\\vec{y} - X\\vec{\\beta})^T(\\vec{y} - X\\vec{\\beta})$$\n",
    "\n",
    "But the Ridge regression equation is $$RSS + \\lambda\\vec{\\beta}^T\\vec{\\beta}$$\n",
    "\n",
    "So doing similar to what we did in proving mulitiple linear regression we expand RSS and get:  \n",
    "\n",
    "$$\\vec{y}^T\\vec{y} - \\vec{y}^TX\\vec{\\beta} - X\\vec{\\beta}^T\\vec{y} + X\\vec{\\beta}^TX\\vec{\\beta} + \\lambda\\vec{\\beta}^T\\vec{\\beta}$$  \n",
    "\n",
    "note: remember how $ \\vec{y}^TX\\vec{\\beta}$ just equals $X\\vec{\\beta}^T\\vec{y}$ because each part of the transpose is just a vector and dot product of 2 vectors no matter the order is the same value. Simplify to get:  \n",
    "\n",
    "$$\\vec{y}^T\\vec{y} - 2X\\vec{\\beta}^T\\vec{y} + X\\vec{\\beta}^TX\\vec{\\beta} + \\lambda\\vec{\\beta}^T\\vec{\\beta}$$  \n",
    "\n",
    "note: $\\frac{\\delta}{\\delta\\vec{\\beta}} \\beta^T\\beta = 2\\beta$. Look at https://github.com/hhprogram/BlogPosts/blob/master/ProofOfVectorDerivatives.ipynb for definition of derivative of a function with an input of a vector. And then take that resulting vector and take the derivative with respect to each $\\beta$ element and you'll get 2$\\beta_i$.\n",
    "$$\\frac{\\delta}{\\delta\\vec{\\beta}} = - 2 X^T\\vec{y} + 2X^TX\\vec{\\beta} + 2\\lambda\\vec{\\beta}$$  \n",
    "Then set to zero and rearrange:  \n",
    "\n",
    "$$2X^TX\\vec{\\beta} + 2\\lambda\\vec{\\beta} = 2 X^T\\vec{y}$$  \n",
    "\n",
    "$$2(X^TX\\vec{\\beta} + \\Lambda)\\vec{\\beta} = 2 X^T\\vec{y}$$ s.t. $\\Lambda$ scalar matrix - i.e diagonals are all $\\lambda$ and rest are zeros  \n",
    "\n",
    "$$\\vec{\\beta} = (X^TX\\vec{\\beta} + \\Lambda)^{-1}X^T\\vec{y}$$\n",
    "\n",
    "\n",
    "\n",
    "$$\\sum_{i=1}^n (y_i - \\beta_0 - \\sum_{j=1}^p \\beta_jx_{ij})^2 + \\lambda\\sum_{j=1}^p\\beta_j^2 = RSS + \\lambda\\sum_{j=1}^p\\beta_j^2$$  \n",
    "\n",
    "You can see that we try to minimize RSS with a penalty term, that penalizes the model for larger values of $\\beta$ (assumiing $\\lambda$ > 0). This constrains the possibilities of the beta values and limits the possible beta values to a smaller subsection. The above 3d graph gives you an idea of what Ridge is doing. It's limiting the possibilites of specific parameter values we can end up with for any training dataset, and thus lowering the variance of $\\hat{f}$ - our predicted function - as we have fewer options to choose form. And lastly, because we will end up with more similar functions $\\hat{f}$ this means the resulting values $\\hat{f}$ will also be more similar in value and therefore have lower variance. This in turn, will drive down the overall MSE as $Var(\\hat{f}(x))$ makes up a part of MSE.   \n",
    "\n",
    "*Note: there is a slight difference between the image as this shows 2 spheres (1) outer sphere has radius of 2 and (2) inner sphere has radius of 1. This actually more accurately represents a more strict constraint where basically the 3 parameter coefficients we are trying to estimate must sum to be less than or equal to 2 or 1, respectively. In the guess of Ridge the polgon would have something less symetrical as it's not a hard constraint just a penalty term*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- Elements of Statisical Learning - Hastie, Tibshirani and Friedman\n",
    "- https://stackoverflow.com/questions/32424670/python-matplotlib-drawing-3d-sphere-with-circumferences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (blogPosts)",
   "language": "python",
   "name": "blogposts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
