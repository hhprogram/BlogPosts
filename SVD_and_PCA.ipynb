{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Notebook tries to go into reasonably amount of depth behind SVD and how it relates to PCA\n",
    "\n",
    "### We'll start with the Singular Value Decomposition defintion.\n",
    "SVD is defined as:\n",
    "\n",
    "$$A = U\\Sigma V^T$$\n",
    "Such that U = m by m orthogonal matrix, V = n by n orthogonal matrix and $\\Sigma$ is a m by n diagonal matrix such that the ith diagonal of $\\Sigma$ is the ith singular value of A. (remember rows by columns when talking about matrices' dimensions). Also, orthogonal matrix - https://en.wikipedia.org/wiki/Orthogonal_matrix. \n",
    "\n",
    "Singular Value of a m by n matrix A - defined as the square root of the eigenvalues of the matrix $A^TA$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- First I'll quickly give a high level reason of using SVD\n",
    "- Second I'll just walk through how to calculate each component of the SVD\n",
    "- Then I'll try my best to explain the reasoning and theory behind some of the calculations we'll see in part 2\n",
    "- Lastly, I'll revisit why we use SVD and what it does for us at a high level and try to connect that with the mathematics behind it and interpret what each matrix 'represents'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD Motivation\n",
    "SVD is generally used to help more efficiently represent data by representing the data in a slightly different manner than its original format. We can then use this different 'format' as either the basic building blocks that can then be linearly combined in such a way to perfectly recreate the original data. Usually, the amount of data these basic building blocks take up is significantly less than the original data and thus we can represent data in a much more efficient manner or SVD will help us 'order' the new representation of the data in a way to help us filter out parts of the data that are insignifcant and thus allowing us to ignore it.  \n",
    "  \n",
    "Some concrete examples of SVD applications are data compression, noise reduction and data analysis. (a more detailed explanation of how SVD is applied here can be found in the first reference bullet).  \n",
    "\n",
    "\n",
    "## SVD Component Calculations\n",
    "There are definitely a few different ways I've seen U calculated, I'm just showing one of them. And probably $\\Sigma$ and V can be calculated a few different ways as well but I'm just going through one way.\n",
    "1.) First we'll calculate the resulting symmetric matrix $A^TA$. We need this matrix in order to calculate the eigenvalues of it to then get the singular values of A.  I'll give an overview of how that part is done.\n",
    "- Find the characteristic equation resulting from det$(A^TA - \\lambda I) = 0$ in order to find the eigenvalues of $A^TA$\n",
    "- Thus now take the square root of each eigenvalue $\\lambda_i$ to get each singular value $\\sigma_i$ which makes up each entry in our diagonal matrix $\\Sigma$\n",
    "2.) Second we need to calculate the resulting eigenvectors of $A^TA$ to get our $V^T$ matrix as the rows of $V^T$ happen to be the eigenvectors of $A^TA$ (I'll show how we know this in the next subsection)\n",
    "- Then for each eigenvalue $\\lambda_i$ find the corresponding bases for each corresponding ith eigenspace. And because the matrix $A^TA$ is symmetric eigenvectors associated with different eigenspaces are orthogonal. And if the dimension of an eigenspace is > 1 we can just use the Gram Schmidt process to make sure those eigenvectors are orthogonal to each other\n",
    "- Then we take those eigenvectors and we normalize them - divide them by its vector length to get orthonormal vectors\n",
    "- The resulting matrix V is an orthogonal matrix. And now we have calculated $V^T$\n",
    "3.) Lastly we need to calculate the U matrix. We can calculate the symmetric matrix $AA^T$ and then basically do the same process of step 2 but for this matrix. The orthonormal eigenvectors of $AA^T$ are now our columns for matrix U.  \n",
    "\n",
    "And there you have it, you have just SVD-ed matrix A.\n",
    "\n",
    "## SVD Component Explanations and Reasoning\n",
    "I remember learning this and being confused on how we knew how and why that U was just the eigenvectors of $AA^T$ and same for V. So here I'll try to shed some light on how the reasoning behind the formation of these 2 matrixes.  \n",
    "1.) Reasoning behind $V$ (and indirectly $V^T$).  \n",
    "- So one way you can think about this is just by leveraging the SVD definition and taking that as your starting point. Then by investigation (side note: not sure exactly how you discover this other than maybe linear algebra intuition and just trying to construct matrices that have good properties or maybe becauase by definition singular values of A are related to matrix $A^TA$ you try calculating $A^TA$ and see what that gives you? Not sure).  \n",
    "  Thus we have $A = U\\Sigma V^T$ then we can do:  \n",
    "  $$A^TA = (U\\Sigma V^T)^TU\\Sigma V^T = V\\Sigma^T U^TU\\Sigma V^T = V\\Sigma^T\\Sigma V^T$$\n",
    "  Note: that $U^TU=I$ because we defined U to be an orthogonal matrix.  \n",
    "  Also, note that $\\Sigma^T\\Sigma V$ is a diagonal matrix as two diagonal matrices multiplied by each other is still diagonal. Then by the Diagonalization theorem (Thm 5 Chapter 5 in the book by Lay, see references), since the theorem is if and only if. And because V is orthogonal thus, $V^T = V^{-1}$ then we can rewrite our final equation as:  $$A^A = V\\Sigma^T\\Sigma V^{-1}$$ and because we can represent matrix A that means that the columns of V must be linearly independent eigenvectors of $A^TA$ - per the diagonlization theorem.  \n",
    "- Or we can also see that $A^TA$ is symetric and be defintion this means that it is orthogonally diagonalizable which then implies that V can be made up of linearly indepedent orthogonal eigenvectors.\n",
    "2.) Then we can apply the same reasoning to the symmetric matrix $AA^T$. If we use the SVD definition and do out the matrix multiplication then we get the same result except with matrix U now. And thus can see that the columns of U are the linearly indepedent eigenvectors of the matrix $AA^T$.  \n",
    "\n",
    "Hopefully that helps explain a little about the formation of each matrix. If there is a more comprenhensive way to go about this I currently am not sure but would be interested to know.\n",
    "\n",
    "## Revisit SVD and Connecting the conceptual understanding of the decomposition to the underlying mathematics\n",
    "This section will go into another way of interpreting the matrices $\\Sigma$ and V. Which I think will help connect how SVD is able to accomplish the things it does I mentioned earlier - in relation to data compression etc. (A lot of this discussion is inspired by reference 3).  \n",
    "\n",
    "CHRIS - still am confused how singular values as defined in reference 3 link as being the value of the objective function and the singular vector being the vector that satisfies the objective function listed right under the 'Singular Values and Vectors' section title why they happen to be the eigenvectors of $A^TA$. Or is it just by definition that singular values are defined as both what I show below and the square root of eigenvalues for $A^TA$\n",
    "\n",
    "Another way to see the matrix V is vectors that we call singular vectors. And each of these vectors are linearly independent from each other and are the resulting vectors when we find a 'k' dimensional subspace to best approximate our original data that lives in some set X.  \n",
    "\n",
    "Let's start with a definition: Given some set $X = {x_1, x_2,...,x_j}$ s.t. $x_i \\in R^N$, then the best approximating 'k dimensional linear subspace' V whose basis can be represented as $V_b = {v_1,..., v_k}$ s.t. $v_i \\in R^N$ is the subspace V that minimizes the squared distances between X and V - i.e minimizes the squared distance from V to every point in X.  \n",
    "\n",
    "If we start with that and remember the simple distance formula between a point a some subspace spanned by some $v_1$ is: $$ \\sum_i (x_i)^2 = d_{v_1}(x)^2 + |proj_v(x)|$$  s.t. $d_{v_1}(x)$ = the distance between some vector 'x' and the subspace spanned by $v_1$ aka V, $proj_v(x)$ = projection of x onto V and vertical bars around it meaning we want to take its magnitude and $\\sum_i (x_i)^2$ = the sum of the squares of each element in vector x. Just the pythagorean theorem.\n",
    "\n",
    "  \n",
    "\n",
    "Then if we rearrange some terms we get the euclidean distance formula of: $$d_{v_1}(x)^2 = \\sum_i (x_i)^2 - |proj_v(x)|$$. And because the x vectors are given (they are just data points that are fixed), they are treated as constants. Thus, in order to minimize the distance $d_{v_1}(x)^2$ we need to maximize the length of the projection $proj_v(x)$. Therefore, the variable in this optimization problem is $v_1$ (we're assuming we just want the best 1-dimensional subspace to approximate X).  \n",
    "\n",
    "Once we solve this optimization we have a $v_1$ that defines the best 1-dimensional linear subspace that approximates X. Then to find the k-dimensional best fitting linear subspace we just do this 'k-1' more times. Each time we find a subspace that is orthogonal to the existing subspace we have found. For example, to find the 2-dimensional best fitting linear subspace we do the same optimization but only consider vectors that are orthogonal to $v_1$ that we found earlier. Than to find a 3-dimensional subspace, we only consider vectors that are orthogonal to the subspace spanned by the set ${v_1, v_2}$ etc.. (There is an induction proof of what this works in reference link 3 below). \n",
    "\n",
    "Lastly, the singular value associated with each singular vector is the actual max value of the |proj_v(x)|. Thus since in the above algorithm we find vectors in decreasing order of max value the bigger singular values are associated with singular vectors that fit the data closer. Thus $v_1$ is a better estimator than $v_2$ and so on if we were to just take one of the vectors. (CHRIS - does this mean |proj_v(x)| = sqrt root of eigenvalues of $A^TA$??)\n",
    "  \n",
    "   \n",
    "In relation to our application. Our data matrix A, is our set X. Thus we are trying to find the best 'k' vectors to approximate each data point in A. And we have now solved for the matrix $\\Sigma$ and U in such a way that we have a better idea of its origins and thus have a better concept of what they represent.  \n",
    "\n",
    "1.) $\\Sigma$ - Is a diagonal matrix populated by the singular values, that come from that optimization of magnitude of projection of all data vectors in X onto the subspace spanned by each column in U. They are ordered in decreasing order, which means the first vector in U is associated with the largest singular value and thus the most descriptive vector for data matrix A.  \n",
    "2.) V - orthogonal matrix consisting of the eigenvectors of $A^TA$. Which for some reason are also the singular vectors we derived from above. Note, that the way we found each vector by construction makes this an orthogonal set. \n",
    "- A little more about this matrix. Each column vector is an n-dimensional vector. Where the vector is some mixture of values of the original 'n' parameter variables, that might be nonsensical in real world terms but give the best 1-d approximation of the original data points. And then there are, usually, 'n' of these (because real world data usually has noise so no singular value will be exactly 0 - first reference link has a little bit deeper discussion in this). There are at most 'n' of these singular vectors because what V is trying to do is find a linear subspace that approximates our original dataset matrix A. And if each data vector / point is on n-dimensional, then that means the data lives in an n-dimensional subspace, thus we reasonably use a maximum of an n-dimensional subspace to approximate an n-dimensional data points (CHRIS - is this right?). \n",
    "\n",
    "3.) U - Another orthogonal matrix consisting of the eigenvectors $AA^T$. Another rotation matrix unless we transform X, See this post for details on what I mean and how to interpret it: https://github.com/hhprogram/BlogPosts/blob/master/Covariance_Eigenvalue_Eigenvectors.ipynb\n",
    "\n",
    "## Last Section - Revisit High Level SVD explanation to the maths\n",
    "Before I said we can use SVD on data compression, noise reduction or data analysis. We can do this by basically decomposing the original data into $U\\Sigma V^T$ and then just having to store the relevant vectors in V, i.e all the vectors in V associated with a non-singular value or associated with a singular value above some threshold. (CHRIS - do we do the same thing for U? I.e take the first 'k' column vectors in U as our relevant data vectors??). Then we can throw out the rest and then just represent all the original data points in terms of some linear combination of those 'k' vectors, saving us space as well as possible eliminating anything that really doesn't help us with our task (i.e explaining a certain response variable etc..) \n",
    "\n",
    "Hopefully that was a helpful deep dive into SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:  \n",
    "1.) http://www.ams.org/publicoutreach/feature-column/fcarc-svd  \n",
    "2.) Linear Algebra and Its Applications (Third Edition) - David C. Lay (used for some Linear Algebra theorems I cite above)  \n",
    "3.) https://jeremykun.com/2016/05/16/singular-value-decomposition-part-2-theorem-proof-algorithm/ (good resource for understanding the background of singular values and why we want to decompose a matrix in this way)  \n",
    "4.) https://www.khanacademy.org/math/linear-algebra/matrix-transformations/lin-trans-examples/v/introduction-to-projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (blogPosts)",
   "language": "python",
   "name": "blogposts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
