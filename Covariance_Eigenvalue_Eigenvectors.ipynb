{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This document will try to go into the details of the important and 'meaning' behind eigenvalues and eigenvectors, especially with respect to covariance matrices. The impetus behind this was trying to gain a deeper understanding of SVD and specifically: \n",
    "\n",
    "(1) why it was that the 'V' matrix in SVD was the eigenvectors of a matrix $X^TX$  \n",
    "(2) why the U was eigenvectors of a matrix $XX^T$ given our original data matrix was X.   \n",
    "(3) why the eigenvalues of both these matrices $X^TX$ and   \n",
    "And also connecting how singular vectors of the matrix X coincide with being the eigenvectors of $X^TX$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "- Explain generally what covariance and variance is and why it might be interesting\n",
    "- Need to mention the way you represent the X data matrix in SVD that leads to different intepretations of the decomposition parts and the meaning of each matrix.\n",
    "- Show how it might be concisely written in matrix form (i.e show how it ends up being $XX^T$ NOTE: covariance matrix is not just $XX^T$ it is only the case if you define X as a matrix whose columns has elements made of each element subtracted by that ith element in every datapoints' average. Ya so this is just a different way of representing your data, so if you represent X as the matrix where you subtract the mean, then yes this is what the covariance matrix is and then we can show how it is by showing how it converts to matrix form easily, then we will talk about what eigenvalues and eigenvectors of a covariance matrix mean - refer to reference (3))\n",
    "- Then revert back to what eigenvalues and eigenvectors for a matrix conceptually mean, at least for a transformation matrix.\n",
    "- Then connect these two somehow? Or just go along with how we are trying to maximize covariance components and to do this we max an equation with some variable vector 'v' and then to optimize you set to 'largest' eigenvector\n",
    "- Then come back and say if when define X as just the original data then 'V' makes conceptual sense and U is just some rotation matrix. And then if X is redefined as data centered around the mean then U is important because of the previous discussion above\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance and Variance\n",
    "- Variance is just a measure of how much variation it contains. In the context of a dataset it means how different is the data, on average how much would a typical point vary from the mean of all the datapoints. Thus, a dataset with high variance has points all over the place or at least far away from the 'mean' (i.e they could be clustered but just at opposite end of whatever metric we measure them on and thus the mean is far away from every point) or with a small variance, each point in the dataset is clustered near one another.\n",
    "- Covariance is a measure of how one piece of a data point changes in relation to another piece of a data point. Or how 2 different metrics or properties move in relation to each other. The variance measures within one aspect of a data point or only one specific matrix (i.e variance of height) but the covariance is a measure of 2 (i.e covariance of height and weight). Therefore, this measure gives you an idea of when one property moves one direction generally does the other variable move with it (positive covariance value), opposite of it (negative covariance value) or totally indepdently / unrelated to it (covariance value near 0). Thus, covariance of height and weight would most likely be positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing variance and covariance in matrix form after redefining my X to be x minus the average of each element within x\n",
    "- Math definition of both:  \n",
    "Variance: $$ \\frac{\\sum(x_i - \\bar{x})^2}{n}$$ s.t. 'n' is the number of points\n",
    "Covariance: $$\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "?? Why exactly eigenvectors show direction that we pull in again? And basically, every linearly indepedent eigenvector is a direction that a matrix is changing the input space to output space right? So, if there happens to be only one linearly independent eigenvector of a transformation matrix - then it only pulls in one direction? - Yes if only one linearly independent eigenvector than only pulling in one direction. It shows the main direction of pull because since when you apply the transformation to that vector the resulting vector doesn't rotate at all and this means that in that specific direction that is the farthest any vector will be pulled in so that is a 'main' direction of transformation. \n",
    "\n",
    "?? Is it right to intepret eigenvectors when we look at a covariance matrix as just another transformation matrix?? I think a way to interpret it as a transformation matrix is to remember that if you're applying a covariance matrix to a vector then that vector will be stretched in the directions of the eigenvectors of that covariance matrix. And those eigenvectors represent the largest 'directions' of variance and thus the most pulling force as if one dimension you pull the data the most vs. all the other dimensions the resulting data will start to vary the most along that dimension. I think that's the best way to connect the two - but I don't know how important it is to make that connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:  \n",
    "(1) http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/   \n",
    "(2) http://www.cse.psu.edu/~rtc12/CSE586Spring2010/lectures/pcaLectureShort_6pp.pdf\n",
    "(3) https://math.stackexchange.com/questions/23596/why-is-the-eigenvector-of-a-covariance-matrix-equal-to-a-principal-component?newreg=8b612908065b4a4b86737b14e050e54f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (blogPosts)",
   "language": "python",
   "name": "blogposts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
