{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This document will try to go into the details of the important and 'meaning' behind eigenvalues and eigenvectors, especially with respect to covariance matrices. The impetus behind this was trying to gain a deeper understanding of SVD and specifically: \n",
    "\n",
    "(1) why it was that the 'V' matrix in SVD was the eigenvectors of a matrix $X^TX$  \n",
    "(2) why the U was eigenvectors of a matrix $XX^T$ given our original data matrix was X.   \n",
    "(3) how covariance matrices are related to SVD and how eigenvectors of these matrices are also related to SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "- What eigenvalues and eigenvectors for a matrix conceptually mean, at least for a transformation matrix.\n",
    "- Explain generally what covariance and variance is and why it might be interesting\n",
    "- The way you represent the X data matrix in SVD that leads to different intepretations of the decomposition parts and the meaning of each matrix.\n",
    "- If represent 'X' as the data points minus the means of each property over all data points then can use the covariance interpretation (U is 'meaningful' matrix)\n",
    "- Show how it might be concisely written in matrix form (i.e show how it ends up being $XX^T$\n",
    "- Then come back and say if when define X as just the original data then 'V' makes conceptual sense and U is just some rotation matrix. And then if X is redefined as data centered around the mean then U is important because of the previous discussion above\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvectors and Transformation Matrices\n",
    "Basically, any matrix can be a transformation matrix. So, if you have some matrix A then it will take some vector $\\vec{x}$ and transform it to a new vector $\\vec{y}$. By definition an eigenvector $\\vec{v}$ satisfies:  \n",
    "\n",
    "$$A\\vec{v} = \\lambda \\vec{v}$$   \n",
    "\n",
    "$\\lambda$ is that eigenvector's corresponding eigenvalue.  \n",
    "\n",
    "So what does it all mean??? Eigenvectors can be thought of as the 'main' directions that the transformation matrix A pulls any input vector $\\vec{x}$ to its resulting output vector $\\vec{y}$. And then any specific resulting $\\vec{y}$ is just some linear combination of these 'main' direcitons, i.e the linearly independent eigenvectors of A. (if A only had one linearly independent eigenvector then all it would do is stretch every $\\vec{x}$ in one direction).  \n",
    "\n",
    "And eigenvalues associated with each eigenvector can be thought of as a relative measure of how 'much' pull in each of these 'main' directions the transformation pulls on any input vector. Thus, eigenvectors with corresponding largest eigenvalues means the transformation matrix A pulls an input vector more in that direction than any other direction. This can be seen since the eigenvalue is a scalar on the eigenvector, thus it a larger scalar on that eigenvector (ie direction) the more the input vector will be acted on that direction. (Reference 4 is good if you want more detail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance and Variance\n",
    "- Variance is just a measure of how much variation it contains. In the context of a dataset it means how different is the data, on average how much would a typical point vary from the mean of all the datapoints. Thus, a dataset with high variance has points all over the place or at least far away from the 'mean' (i.e they could be clustered but just at opposite end of whatever metric we measure them on and thus the mean is far away from every point) or with a small variance, each point in the dataset is clustered near one another.\n",
    "- Covariance is a measure of how one piece of a data point changes in relation to another piece of a data point. Or how 2 different metrics or properties move in relation to each other. The variance measures within one aspect of a data point or only one specific matrix (i.e variance of height) but the covariance is a measure of 2 (i.e covariance of height and weight). Therefore, this measure gives you an idea of when one property moves one direction generally does the other variable move with it (positive covariance value), opposite of it (negative covariance value) or totally indepdently / unrelated to it (covariance value near 0). Thus, covariance of height and weight would most likely be positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing variance and covariance in matrix form after redefining my X to be x minus the average of each element within x\n",
    "- Math definition of both:  \n",
    "Variance: $$ \\frac{\\sum(x_i - \\bar{x})^2}{n}$$ s.t. 'n' is the number of points  \n",
    "\n",
    "Covariance of the jth parameter of each datapoint with another ith parameter of every datapoint. When i=j then that's just the variance of the jth parameter: $$\\frac{\\sum_n (x_{i} - \\bar{x_i})(x_{j} - \\bar{x_j})}{n}$$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNDER CONSTRUCTION: Some vector/matrix stuff that is confusing me but should be trival to show\n",
    "\n",
    "This can be transformed into matrix form by making some new definitions. If we define a vector $\\bar{x}$ to be:   $$\\bar{x} = \\begin{bmatrix}\n",
    "                                       \\frac{\\sum_i x_{i,1}}{n} \\\\\n",
    "                                       \\frac{\\sum_i x_{i,2}}{n} \\\\\n",
    "                                       \\vdots \\\\\n",
    "                                       \\frac{\\sum_i x_{i,p}}{n}\n",
    "                                  \\end{bmatrix}$$\n",
    "s.t. $\\frac{\\sum_i x_{i,1}}{n}$ is the average value for the 1st parameter (i.e independent variable) over all 'n' data points. \n",
    "\n",
    "Then we can define a matrix whose columns are column vectors of the form $x_i - \\bar{x}$ where $x_i$ is just the column vector of an original data point 'i' with dimension 'p' (where 'p' is the number parameters we take into consideration for each data point) and $\\bar{x}$ is the column vector we just defined above. Thus we have matrix $X_m$ (using m to denote centered around the mean now):  \n",
    "\n",
    "$$X_m  = \\begin{bmatrix}\n",
    "                                       x_1 - \\bar{x_1},\n",
    "                                       x_2 - \\bar{x_2}, ...\n",
    "                                       ,\n",
    "                                       x_p - \\bar{x_p}\n",
    "                                  \\end{bmatrix}$$  \n",
    "                                  \n",
    "$$X_mX_m^T = \\begin{bmatrix}\n",
    "                                      \\frac{x_{1,1} - \\bar{x_{1}}}{n} + \\frac{x_{1,2} - \\bar{x_{2}}}{n} + .. + \\frac{x_{1,p} - \\bar{x_{p}}}{n} \\\\\n",
    "                                       \\frac{x_{2,1} - \\bar{x_{1}}}{n} + \\frac{x_{2,2} - \\bar{x_{2}}}{n} + .. + \\frac{x_{2,p} - \\bar{x_{p}}}{n} \\\\\n",
    "                                       \\vdots \\\\\n",
    "                                       \\frac{x_{n,1} - \\bar{x_{1}}}{n} + \\frac{x_{n,2} - \\bar{x_{2}}}{n} + .. + \\frac{x_{n,p} - \\bar{x_{p}}}{n}\n",
    "                                  \\end{bmatrix}$$\n",
    "UNDER CONSTRUCTION AREA ENDS HERE..\n",
    "                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: When we subtract the mean of each parameter from each data point, we are just moving the origin of each dimension to the mean of each parameter rather than around 0. Doesn't impact the results in any meaningful way - just a different way to represent the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have represented the data X as $X_m$, and we can just treat $X_m$ basically like it was the original data matrix and we have shown that $X_mX_m^T$ is the covariance matrix of $X$. Now we'll show why the eigenvectors of this matrix $X_mX_m^T$ correspond to dimensions that have the highest variance (i.e dimensions that hold the most information for each datapoint)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using this new $X_m$ to help us understand importance of the covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's say we only are allowed to describe any datapoint that had an original 'p' dimensions in only one dimension. We'd want to choose a dimension that holds the most information, or otherwise differentiates the datapoints the most. For examople, if we were trying to differentiate all green clothing items apart, we wouldn't pick the dimension color as our one dimension because then all the datapoints would look the same.  \n",
    "\n",
    "In more mathy speak (i think) we basically want to choose some vector $\\vec{u}$ such that the projection of our data matrix X onto it has the highest variance. Therefore, if we project each datapoint onto this vector each datapoint will now just we some new vector that lies along $\\vec{u}$ with some magnitude $\\vec{u} \\cdot \\vec{x_i}$. Or if we constrain this $\\vec{u}$ to be a unit vector then our new datapoint is just a projection that is equal to $(\\vec{u} \\cdot \\vec{x_i})\\vec{u}$ (see reference 4 for full review of projections). Therefore, we want to choose a $\\vec{u}$ that maxmizes the variance of these projected datapoints and the variance can be shown (by some magic I don't fully understand yet) to be $\\vec{u}^T\\Sigma\\ \\vec{u}$ where $\\Sigma$ is the covariance matrix of our original data matrix X. Told you the covariance matrix would be important! Then by some more magic (that I don't understand yet either but plan to revisit and add to here) is given this equation with variable $\\vec{u}$ this just solves to being $\\vec{u}$ = eigenvector with the largest eigenvalue of the matrix $\\Sigma$.  \n",
    "\n",
    "Ok - so what does that show? Well we wanted to pick one dimension (now remember this one dimension may not really be a sensical dimension in terms of real-world dimensions go. It's probably some weird linear combination of a bunch of the dimensions but mathematically it holds the most info over all the data points and when you project all the datapoints onto it, the resulting projected points have the most variance) that held the most information - and consequently the highest variance. We then showed how the covariance matrix of X and the resulting eigenvectors and eigenvalues lead us to that vector / dimension.  \n",
    "\n",
    "There's a little more magic that happens that now if you wanted to keep going to get another dimension that holds the 2nd most information, you would then subtract out the vector we found above from the data and then repeat the process. But it turns out each resulting vector is just the next eigenvector associated with the next highest eigenvalue. (plan to return to flesh out some of the details here a little more).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relating it back to $X = U\\Sigma V^T$  \n",
    "So what does that mean? Well remembering the SVD definition and how to find the columns of U. The columns of U are the eigenvectors of a matrix $XX^T$. Well, this matrix doesn't really have a meaningful intepretation (other than a rotation matrix) if X is the original data - in this case V is the more 'meaningful' matrix (back to that a little later), BUT if we redefine X to be $X_m$, where $X_m$ is the data matrix with each parameter mean subtracted then now we have $X_m = UDV^T$ but now $X_mX_m^T$ is the covariance matrix and now U is the 'meaningful' matrix and V is just another rotation matrix. Why is U meaningful? Because of the discussion we have above, if U is made of the eigenvectors of $X_mX_m^T$ and $X_mX_m^T$ is the covariance matrix of the original data matrix X - the work we just showed above shows us that that means that each eignevector (i.e column of U) are the dimensions that have the highest variance amongst the original data points. Or in other words are ordered vectors holding the most information about each datapoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about V now though?\n",
    "So on another post (https://github.com/hhprogram/BlogPosts/blob/master/SVD_and_PCA.ipynb) I talk about how V can be conceptually interpreted. This is still true, however it's only really applicable if you maintain X to be the original data matrix. Then I would interpret the SVD by saying the matrices $\\Sigma$ are the singular values that tell me which singular vectors in V are the most descriptive of X. However, if we transform X to $X_m$ then I would use the above reasoning and say $\\Sigma$ are the eigenvalues that tell me which eigenvectors in U are the most important dimensions describing X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully this helped clear up any confusion related to covariance matrices, eigenvectors/eigenvalues and how they might be related and important in relation to SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:  \n",
    "(1) http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/   \n",
    "(2) http://www.cse.psu.edu/~rtc12/CSE586Spring2010/lectures/pcaLectureShort_6pp.pdf  \n",
    "(3) https://math.stackexchange.com/questions/23596/why-is-the-eigenvector-of-a-covariance-matrix-equal-to-a-principal-component?newreg=8b612908065b4a4b86737b14e050e54f  \n",
    "(4) https://www.youtube.com/watch?v=cZuDWviSI4c  \n",
    "(5) https://skymind.ai/wiki/eigenvector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions and answers to potentially delete:\n",
    "\n",
    "?? Why exactly eigenvectors show direction that we pull in again? And basically, every linearly indepedent eigenvector is a direction that a matrix is changing the input space to output space right? So, if there happens to be only one linearly independent eigenvector of a transformation matrix - then it only pulls in one direction? - Yes if only one linearly independent eigenvector than only pulling in one direction. It shows the main direction of pull because since when you apply the transformation to that vector the resulting vector doesn't rotate at all and this means that in that specific direction that is the farthest any vector will be pulled in so that is a 'main' direction of transformation. \n",
    "\n",
    "?? Is it right to intepret eigenvectors when we look at a covariance matrix as just another transformation matrix?? I think a way to interpret it as a transformation matrix is to remember that if you're applying a covariance matrix to a vector then that vector will be stretched in the directions of the eigenvectors of that covariance matrix. And those eigenvectors represent the largest 'directions' of variance and thus the most pulling force as if one dimension you pull the data the most vs. all the other dimensions the resulting data will start to vary the most along that dimension. I think that's the best way to connect the two - but I don't know how important it is to make that connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (blogPosts)",
   "language": "python",
   "name": "blogposts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
